{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cover_Type的值域:  [5 2 1 7 3 6 4]\n",
      "\n",
      "每个值的大小:\n",
      "Cover_Type\n",
      "5    2000\n",
      "2    2000\n",
      "1    2000\n",
      "7    2000\n",
      "3    2000\n",
      "6    2000\n",
      "4    2000\n",
      "Name: count, dtype: int64\n",
      "Generated 16 bits private key:  d751\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "# show info of dataset\n",
    "import pandas as pd\n",
    "\n",
    "# 数据集文件路径\n",
    "file_path = '/home/zhengyihao/dataset/covertype/covtype_with_key.subset.data'\n",
    "\n",
    "# # 根据所提供的信息，创建一个包含所有列名的列表\n",
    "# column_names = [\n",
    "#     'Elevation', 'Aspect', 'Slope',\n",
    "#     'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n",
    "#     'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n",
    "#     'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points'\n",
    "# ]\n",
    "\n",
    "# # 添加Wilderness_Area的4个二进制列\n",
    "# wilderness_areas = ['Wilderness_Area' + str(i) for i in range(1, 5)]\n",
    "# column_names += wilderness_areas\n",
    "\n",
    "# # 添加Soil_Type的40个二进制列\n",
    "# soil_types = ['Soil_Type' + str(i) for i in range(1, 41)]\n",
    "# column_names += soil_types\n",
    "\n",
    "# # 添加Cover_Type列\n",
    "# column_names.append('Cover_Type')\n",
    "\n",
    "# 使用列名列表读取数据\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 获取Cover_Type的值域（唯一值）\n",
    "unique_values = data['Cover_Type'].unique()\n",
    "\n",
    "# 获取每个值的计数\n",
    "value_counts = data['Cover_Type'].value_counts()\n",
    "\n",
    "# 打印结果\n",
    "print(\"Cover_Type的值域: \", unique_values)\n",
    "print(\"\\n每个值的大小:\")\n",
    "print(value_counts)\n",
    "\n",
    "\n",
    "\n",
    "# # 在原始数据上加上 primary key列\n",
    "# # 通过reset_index函数为数据集添加索引列，这列就是“primary_key”\n",
    "# data = data.reset_index()\n",
    "\n",
    "# # 设置新的列名，将之前的index变为 primary_key\n",
    "# data = data.rename(columns={'index':'primary_key'})\n",
    "\n",
    "# # 将新的数据帧保存到硬盘上\n",
    "# data.to_csv('/home/zhengyihao/dataset/covertype/covtype_with_key.data', index=False)\n",
    "\n",
    "\n",
    "\n",
    "# 定义用户私钥 Ks\n",
    "import secrets\n",
    "\n",
    "# 生成 16 bits 的随机密钥\n",
    "Ks = secrets.token_hex(2)\n",
    "\n",
    "print(\"Generated 16 bits private key: \", Ks)\n",
    "\n",
    "\n",
    "\n",
    "# 定义水印长度 l 与组数 N_g（数值相同）\n",
    "l = 400\n",
    "N_g = 400\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分组\n",
    "import hashlib\n",
    "\n",
    "def hash_function(Ks, PK, N_g):\n",
    "    # 使用sha256作为哈希函数\n",
    "    hash_obj = hashlib.sha256()\n",
    "    # 计算H(Ks|tu.PK)\n",
    "    inner_value = (str(Ks) + str(PK)).encode('utf-8')\n",
    "    hash_obj.update(inner_value)\n",
    "    inner_hash = hash_obj.hexdigest()\n",
    "    # 计算H(Ks|H(Ks|tu.PK))\n",
    "    outer_value = (str(Ks) + inner_hash).encode('utf-8')\n",
    "    hash_obj.update(outer_value)\n",
    "    outer_hash = hash_obj.hexdigest()\n",
    "    # 组别序号\n",
    "    group_number = int(outer_hash, 16) % N_g\n",
    "    return group_number\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 加载数据集\n",
    "data = pd.read_csv('/home/zhengyihao/dataset/covertype/covtype_with_key.subset.data')\n",
    "\n",
    "# 应用 hash 函数\n",
    "data['group_number'] = data['primary_key'].apply(lambda x: hash_function(Ks, x, N_g))\n",
    "\n",
    "# 按 group_number 排序重置\n",
    "sorted_data = data.sort_values(by='group_number')\n",
    "\n",
    "# 将结果保存到新的 CSV 文件\n",
    "sorted_data.to_csv('/home/zhengyihao/dataset/covertype/covtype_with_group.data', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_number\n",
      "0      38\n",
      "1      48\n",
      "2      46\n",
      "3      34\n",
      "4      30\n",
      "       ..\n",
      "395    34\n",
      "396    36\n",
      "397    40\n",
      "398    35\n",
      "399    38\n",
      "Length: 400, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "# 加载数据集\n",
    "data = pd.read_csv('/home/zhengyihao/dataset/covertype/covtype_with_group.data')\n",
    "\n",
    "# 统计每个组的 tuples 的数量\n",
    "group_counts = data.groupby('group_number').size()\n",
    "\n",
    "# 打印统计结果\n",
    "print(group_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 嵌入水印"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "要嵌入的水印信息是 1011110101000101001100100010111011111010101000101101010011100001111001100001010001101001101010110001000010110111001001000000000010010101010111101010110100010010111011100111010000110000000000111101011011001101001010100001100001110111010100100100010011011010000000100011010101100110001000000010101000111010110110011100111001101101111101001001101101100010010100101000100100111101111100000011010011010101\n",
      "Count of 1:  187\n",
      "Count of 0:  213\n",
      "水印信息已保存到 histogram_mark.txt\n"
     ]
    }
   ],
   "source": [
    "# 生成嵌入水印信息\n",
    "\n",
    "import random\n",
    "\n",
    "# 生成一个长度为 400 的字符串，它由随机的 0 和 1 组成\n",
    "watermark = ''.join(random.choice('01') for _ in range(400))\n",
    "# 打印二进制字符串\n",
    "print(\"要嵌入的水印信息是\",watermark)\n",
    "\n",
    "# 统计二进制字符串中 1 的个数\n",
    "count_1 = watermark.count('1')\n",
    "\n",
    "# 统计二进制字符串中 0 的个数\n",
    "count_0 = watermark.count('0')\n",
    "\n",
    "# 打印结果\n",
    "print(\"Count of 1: \", count_1)\n",
    "print(\"Count of 0: \", count_0)\n",
    "\n",
    "# 保存水印信息到文件\n",
    "with open(\"histogram_mark.txt\", \"w\") as file:\n",
    "    file.write(watermark)\n",
    "\n",
    "print(\"水印信息已保存到 histogram_mark.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# 初始化 group_number\n",
    "group_number = 0\n",
    "_max = data['Cover_Type'].max()\n",
    "_min = data['Cover_Type'].min()\n",
    "\n",
    "# 创建 y_hat 列\n",
    "data['y_hat'] = (_max + _min) / 2\n",
    "pa = {}\n",
    "mp = []\n",
    "\n",
    "for bit in watermark:\n",
    "    # 选取当前 group 的数据\n",
    "    mask = data['group_number'] == group_number\n",
    "    group_data = data[mask]\n",
    "\n",
    "    # 计算 p_e (忽略 min 和 max)\n",
    "    mask2 = (group_data['Cover_Type'] != _max) & (group_data['Cover_Type'] != _min)\n",
    "    p_e = group_data.loc[mask2, 'Cover_Type'] - group_data.loc[mask2, 'y_hat']\n",
    "\n",
    "    # 使用 Counter 统计每个 p_e 出现的频数\n",
    "    counter = Counter(np.abs(p_e))\n",
    "\n",
    "    # 找出出现频数最高的 p_e 并赋值给 p\n",
    "    p = counter.most_common(1)[0][0]\n",
    "\n",
    "    # 把当前 group 中, Cover_Type 为 min 或 max 的 primary_key 存入 mp\n",
    "    mask_min_or_max = (group_data['Cover_Type'] == _max) | (group_data['Cover_Type'] == _min)\n",
    "    mp.extend(group_data.loc[mask_min_or_max, 'primary_key'].values.tolist())\n",
    "\n",
    "    # 对于那些不等于 min 或 max 的 'Cover_Type'，更新原始数据集数据\n",
    "    mask &= mask2\n",
    "\n",
    "    group_cover_type = data.loc[mask, 'Cover_Type']\n",
    "    y_hat = (_max + _min) / 2\n",
    "    p_e = group_cover_type - y_hat\n",
    "\n",
    "    # 更新 p_e 的值\n",
    "    p_e = np.where((p_e == p) & (bit == '0'), p_e,\n",
    "                   np.where((p_e == p) & (bit == '1'), p_e + 1,\n",
    "                            np.where((p_e == -p) & (bit == '0'), p_e,\n",
    "                                     np.where((p_e == -p) & (bit == '1'), p_e - 1,\n",
    "                                              np.where(p_e >= p + 1, p_e + 1,\n",
    "                                                       np.where(p_e <= -(p + 1), p_e - 1, p_e))))))\n",
    "\n",
    "    # 计算 y_prime，并更新 'Cover_Type'\n",
    "    data.loc[mask, 'Cover_Type'] = p_e + y_hat\n",
    "\n",
    "    pa[group_number] = p\n",
    "\n",
    "    # 更新 group_number 到下一组\n",
    "    group_number += 1\n",
    "\n",
    "# 将字典 pa 转变为 DataFrame 对象并保存为 .csv 文件\n",
    "df_pa = pd.DataFrame(list(pa.items()), columns=['Number', 'Value'])\n",
    "df_pa.to_csv('histogram_pa.csv', index=False)\n",
    "\n",
    "# 将列表 mp 转变为 DataFrame 对象并保存为 .csv 文件\n",
    "df_mp =pd.DataFrame(mp, columns=['Key'])\n",
    "df_mp.to_csv('histogram_mp.csv', index=False)\n",
    "\n",
    "data.to_csv('/home/zhengyihao/dataset/covertype/histogrammark_covertype.data.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cover_Type的值域:  [7 4 1 3 6 2 5]\n",
      "\n",
      "每个值的大小:\n",
      "Cover_Type\n",
      "1    4052\n",
      "7    4042\n",
      "4    1858\n",
      "6    1050\n",
      "5    1050\n",
      "2    1040\n",
      "3     908\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/zhengyihao/dataset/covertype/histogrammark_covertype.data.csv'\n",
    "\n",
    "# 使用列名列表读取数据\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 获取Cover_Type的值域（唯一值）\n",
    "unique_values = data['Cover_Type'].unique()\n",
    "\n",
    "# 获取每个值的计数\n",
    "value_counts = data['Cover_Type'].value_counts()\n",
    "\n",
    "# 打印结果\n",
    "print(\"Cover_Type的值域: \", unique_values)\n",
    "print(\"\\n每个值的大小:\")\n",
    "print(value_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取水印"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# count bit = 0\u001b[39;00m\n\u001b[1;32m     24\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# count bit = 1\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _,row \u001b[38;5;129;01min\u001b[39;00m group_data\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     26\u001b[0m     pe \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCover_Type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m y_hat\n\u001b[1;32m     27\u001b[0m     p \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mget(group_number)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/watermark/lib/python3.10/site-packages/pandas/core/frame.py:1542\u001b[0m, in \u001b[0;36mDataFrame.iterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1540\u001b[0m using_cow \u001b[38;5;241m=\u001b[39m using_copy_on_write()\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[0;32m-> 1542\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m using_cow \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mis_single_block:\n\u001b[1;32m   1544\u001b[0m         s\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39madd_references(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/watermark/lib/python3.10/site-packages/pandas/core/series.py:594\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    592\u001b[0m NDFrame\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data)\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m--> 594\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_pandas_object \u001b[38;5;129;01mand\u001b[39;00m data_dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m data_dtype:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/watermark/lib/python3.10/site-packages/pandas/core/generic.py:813\u001b[0m, in \u001b[0;36mNDFrame._set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;124;03mThis is called from the cython code when we set the `index` attribute\u001b[39;00m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;124;03mdirectly, e.g. `series.index = [1, 2, 3]`.\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    812\u001b[0m labels \u001b[38;5;241m=\u001b[39m ensure_index(labels)\n\u001b[0;32m--> 813\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/watermark/lib/python3.10/site-packages/pandas/core/internals/managers.py:238\u001b[0m, in \u001b[0;36mBaseBlockManager.set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: AxisInt, new_labels: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_set_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis] \u001b[38;5;241m=\u001b[39m new_labels\n",
      "File \u001b[0;32m/opt/anaconda3/envs/watermark/lib/python3.10/site-packages/pandas/core/internals/base.py:89\u001b[0m, in \u001b[0;36mDataManager._validate_set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_set_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: AxisInt, new_labels: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     old_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m[\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     new_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(new_labels)\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;66;03m# If we are setting the index on a DataFrame with no columns,\u001b[39;00m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;66;03m#  it is OK to change the length.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/watermark/lib/python3.10/site-packages/pandas/core/indexes/base.py:909\u001b[0m, in \u001b[0;36mIndex.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    900\u001b[0m         c\n\u001b[1;32m    901\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munique(level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[: get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay.max_dir_items\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m    902\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m c\u001b[38;5;241m.\u001b[39misidentifier()\n\u001b[1;32m    903\u001b[0m     }\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;66;03m# Array-Like Methods\u001b[39;00m\n\u001b[1;32m    907\u001b[0m \n\u001b[1;32m    908\u001b[0m \u001b[38;5;66;03m# ndarray compat\u001b[39;00m\n\u001b[0;32m--> 909\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    910\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;124;03m    Return the length of the Index.\u001b[39;00m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "#提取水印\n",
    "file_path = '/home/zhengyihao/dataset/covertype/histogrammark_covertype.data.csv'\n",
    "detected_data = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "\n",
    "_max = detected_data['Cover_Type'].max()\n",
    "_min = detected_data['Cover_Type'].min()\n",
    "y_hat = (_max + _min) / 2\n",
    "\n",
    "group_number = 0\n",
    "\n",
    "W_det = \"\"\n",
    "\n",
    "for bit in watermark:\n",
    "    w_det = 0\n",
    "    mask = detected_data['group_number'] == group_number\n",
    "    group_data = detected_data[mask]\n",
    "    a = 0 # count bit = 0\n",
    "    b = 0 # count bit = 1\n",
    "    for _,row in group_data.iterrows():\n",
    "        pe = row['Cover_Type'] - y_hat\n",
    "        p = pa.get(group_number)\n",
    "        if row['primary_key'] not in mp:\n",
    "            if pe == p:\n",
    "                w_det = 0\n",
    "            elif pe == p+1 or pe == p-1:\n",
    "                w_det = 1\n",
    "\n",
    "            if w_det == 1:\n",
    "                b += 1\n",
    "            else:\n",
    "                a += 1\n",
    "    if a > b:\n",
    "        w_det = 0\n",
    "    else:\n",
    "        w_det = 1\n",
    "    # print(w_det)\n",
    "    W_det += str(w_det)\n",
    "\n",
    "    group_number += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1011110111001111101101101110111111111010111100111101010011101111111011101101010011101101111010110111110010110111001101100001000010110111110111101110110111110110111011100111110000111001000110111101011011101101011010100011111001110111011100101100011011011010010001101011010111110111001001011111101100111011110110011100111011101101111101111001101101101110011101101000100100111111111100010011110011010101\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_path = '/home/zhengyihao/dataset/covertype/histogrammark_covertype.data.csv'\n",
    "detected_data = pd.read_csv(file_path)\n",
    "\n",
    "# 计算 y_hat\n",
    "_max = detected_data['Cover_Type'].max()\n",
    "_min = detected_data['Cover_Type'].min()\n",
    "y_hat = (_max + _min) / 2\n",
    "\n",
    "# 计算 pe 值\n",
    "detected_data['pe'] = detected_data['Cover_Type'] - y_hat\n",
    "\n",
    "# 将原有的list类型转化为集合数据类型，提高在其中查找项的速度\n",
    "mp_set = set(mp)\n",
    "W_det = \"\"\n",
    "\n",
    "for group_number, bit in enumerate(watermark):\n",
    "    # 对当前组进行操作\n",
    "    group_data = detected_data[detected_data['group_number'] == group_number]\n",
    "    p = pa[group_number]\n",
    "\n",
    "    a = 0 # count bit = 0\n",
    "    b = 0 # count bit = 1\n",
    "\n",
    "    # 通过将一组条件（每行是否满足要求）应用于数据框并进行求和，避免了逐行运算\n",
    "    mask = ~group_data['primary_key'].isin(mp_set) & ((group_data['pe'] == p+1) | (group_data['pe'] == p-1))\n",
    "    b = mask.sum()\n",
    "    mask = ~group_data['primary_key'].isin(mp_set) & (group_data['pe'] == p)\n",
    "    a = mask.sum()\n",
    "\n",
    "    W_det += '0' if a > b else '1'\n",
    "print(W_det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W:  1011110101000101001100100010111011111010101000101101010011100001111001100001010001101001101010110001000010110111001001000000000010010101010111101010110100010010111011100111010000110000000000111101011011001101001010100001100001110111010100100100010011011010000000100011010101100110001000000010101000111010110110011100111001101101111101001001101101100010010100101000100100111101111100000011010011010101\n",
      "W_det:  1011110111001111101101101110111111111010111100111101010011101111111011101101010011101101111010110111110010110111001101100001000010110111110111101110110111110110111011100111110000111001000110111101011011101101011010100011111001110111011100101100011011011010010001101011010111110111001001011111101100111011110110011100111011101101111101111001101101101110011101101000100100111111111100010011110011010101\n",
      "Mismatch Percentage: 17.75%\n"
     ]
    }
   ],
   "source": [
    "#计算 BER\n",
    "def mismatch_percentage(W, W_det):\n",
    "    # 首先检查 W 和 W_det 是否长度相同\n",
    "    if len(W) != len(W_det):\n",
    "        print('Error: The lengths of W and W_det are not the same!')\n",
    "        return\n",
    "    # 计算不匹配度\n",
    "    count_mismatch = sum(a != b for a, b in zip(W, W_det))\n",
    "    percentage = count_mismatch / len(W)\n",
    "    return percentage\n",
    "\n",
    "# W = '1010101010'\n",
    "# W_det = '1010001010'\n",
    "print(\"W: \",watermark)\n",
    "print(\"W_det: \",W_det)\n",
    "print(f'Mismatch Percentage: {mismatch_percentage(watermark, W_det):.2%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watermark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
